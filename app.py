import streamlit as st
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate
# UPDATED IMPORTS to use the latest packages and avoid deprecation warnings
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import Ollama

# --- CONFIGURATION ---
# Purpose: Define constants for the application.
DB_PATH = "db"
MODEL_NAME = "mistral"
EMBEDDING_MODEL = "BAAI/bge-base-en-v1.5"

# --- RAG CHAIN SETUP ---
# Concept: Caching. Streamlit reruns the script on each interaction.
# The `@st.cache_resource` decorator tells Streamlit to run this function only once
# and then store the result in memory (cache it). This prevents us from reloading
# the models and database on every single chat message, which would be very slow.
@st.cache_resource
def load_rag_chain():
    """Loads the entire RAG chain and returns it."""
    print("Loading the RAG chain...")
    # Initialize the same embedding model used to build the database
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    
    # Load the ChromaDB database from disk
    vectordb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    
    # Initialize the Ollama LLM
    llm = Ollama(model=MODEL_NAME)
    
    # Create the retriever
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})
    
    # Create the prompt template
    template = """
    You are a helpful and knowledgeable assistant for the Thakur College of Engineering and Technology (TCET).
    Your goal is to provide detailed and comprehensive answers based only on the context provided.
    Do not make up information. If the context does not contain the answer, say so clearly.

    Based on the following context, please provide a detailed answer to the question.

    Context:
    {context}

    Question:
    {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    
    # Create and return the RAG chain
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    print("RAG chain loaded successfully.")
    return chain

# --- STREAMLIT UI SETUP ---
st.set_page_config(page_title="TCET Chatbot", page_icon="ðŸ¤–")
st.title("ðŸ¤– TCET College Assistant")
st.caption("Your friendly AI guide to Thakur College of Engineering and Technology")

# Concept: Session State. This is Streamlit's way of storing variables
# that should persist as the user interacts with the app. We use it here
# to remember the chat history.
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Welcome! How can I help you with your questions about TCET today?"}
    ]

# Display the chat history
for message in st.session_state.messages:
    # `st.chat_message` creates a chat bubble in the UI.
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Load the RAG chain (this will be cached after the first run)
rag_chain = load_rag_chain()

# --- CHAT INTERACTION LOGIC ---
# `st.chat_input` creates the text box at the bottom of the screen.
# The `if prompt := ...` syntax assigns the user's input to the `prompt` variable
# and runs the following code block only when the user presses Enter.
if prompt := st.chat_input("Ask me anything about TCET..."):
    # Add the user's message to the chat history and display it.
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Display the assistant's response.
    with st.chat_message("assistant"):
        # `st.spinner` shows a loading message while the chain is running.
        with st.spinner("Thinking..."):
            # `st.write_stream` is used to display the output of the RAG chain
            # chunk by chunk as it's generated by the LLM, creating a "typing" effect.
            response = st.write_stream(rag_chain.stream(prompt))

    # Add the assistant's final response to the chat history.
    st.session_state.messages.append({"role": "assistant", "content": response})